{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、配置环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZHIPUAI_API_KEY\n",
      "DASHSCOPE_API_KEY\n"
     ]
    }
   ],
   "source": [
    "env_variables = dotenv_values('.env')\n",
    "for var in env_variables:\n",
    "    print(var)\n",
    "    os.environ[var] = env_variables[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-f7e5ff2257cd4cff95eaedffb9ddb35e'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、配置LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from zhipuai import ZhipuAI\n",
    "import dashscope\n",
    "import random\n",
    "from http import HTTPStatus\n",
    "import dashscope\n",
    "from dashscope import Generation  # 建议dashscope SDK 的版本 >= 1.14.0\n",
    "\n",
    "PROMPT_TEMPLATE = dict(\n",
    "    RAG_PROMPT_TEMPALTE=\"\"\"使用以上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\n",
    "        问题: {question}\n",
    "        可参考的上下文：\n",
    "        ···\n",
    "        {context}\n",
    "        ···\n",
    "        如果给定的上下文无法让你做出回答，请回答数据库中没有这个内容，你不知道。\n",
    "        有用的回答:\"\"\",\n",
    "    InternLM_PROMPT_TEMPALTE=\"\"\"先对上下文进行内容总结,再使用上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\n",
    "        问题: {question}\n",
    "        可参考的上下文：\n",
    "        ···\n",
    "        {context}\n",
    "        ···\n",
    "        如果给定的上下文无法让你做出回答，请回答数据库中没有这个内容，你不知道。\n",
    "        有用的回答:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self, path: str = '') -> None:\n",
    "        self.path = path\n",
    "\n",
    "    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n",
    "        pass\n",
    "\n",
    "    def load_model(self):\n",
    "        pass\n",
    "\n",
    "class GLM4Chat(BaseModel):\n",
    "    def __init__(self, path: str = '', model: str = \"glm-4\") -> None:\n",
    "        super().__init__(path)\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n",
    "        client = ZhipuAI(api_key=os.getenv(\"ZHIPUAI_API_KEY\"))  # 填写您自己的APIKey\n",
    "        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"glm-4\",  # 填写需要调用的模型名称\n",
    "            messages=history\n",
    "        )\n",
    "        return response.choices[0].message\n",
    "\n",
    "class QwenChat(BaseModel):\n",
    "    def __init__(self, path: str = '', model: str = \"qwen-turbo\") -> None:\n",
    "        super().__init__(path)\n",
    "        dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n",
    "        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n",
    "        response = Generation.call(model=\"qwen-turbo\",\n",
    "                                messages=history,\n",
    "                                # 设置随机数种子seed，如果没有设置，则随机数种子默认为1234\n",
    "                                seed=random.randint(1, 10000),\n",
    "                                # 将输出设置为\"message\"格式\n",
    "                                result_format='message')\n",
    "        if response.status_code == HTTPStatus.OK:\n",
    "            return response.output.choices[0].message[\"content\"]\n",
    "        else:\n",
    "            print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n",
    "                response.request_id, response.status_code,\n",
    "                response.code, response.message\n",
    "            ))\n",
    "\n",
    "\n",
    "class OpenAIChat(BaseModel):\n",
    "    def __init__(self, path: str = '', model: str = \"gpt-3.5-turbo-1106\") -> None:\n",
    "        super().__init__(path)\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()   \n",
    "        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=history,\n",
    "            max_tokens=150,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "class InternLMChat(BaseModel):\n",
    "    def __init__(self, path: str = '') -> None:\n",
    "        super().__init__(path)\n",
    "        self.load_model()\n",
    "\n",
    "    def chat(self, prompt: str, history: List = [], content: str='') -> str:\n",
    "        prompt = PROMPT_TEMPLATE['InternLM_PROMPT_TEMPALTE'].format(question=prompt, context=content)\n",
    "        response, history = self.model.chat(self.tokenizer, prompt, history)\n",
    "        return response\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        import torch\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.path, torch_dtype=torch.float16, trust_remote_code=True).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！有什么我可以帮助你的吗？'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwenchat = QwenChat()\n",
    "qwenchat.chat(history=[], prompt=\"hello\", content=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、设置知识库FaissVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import json\n",
    "\n",
    "import faiss\n",
    "\n",
    "from RAG.Embeddings import BaseEmbeddings, OpenAIEmbedding, JinaEmbedding, ZhipuEmbedding\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from RAG.VectorBase import VectorStore\n",
    "\n",
    "\n",
    "class FaissVectorStore(VectorStore):\n",
    "\n",
    "    def __init__(self, document: List[str] = ['']) -> None:\n",
    "        super().__init__(document)\n",
    "        self.document = document\n",
    "\n",
    "    def get_vector(self, EmbeddingModel: BaseEmbeddings) -> List[List[float]]:\n",
    "        self.vectors = []\n",
    "        for doc in tqdm(self.document, desc=\"Calculating embeddings\"):\n",
    "            self.vectors.append(EmbeddingModel.get_embedding(doc))\n",
    "        return self.vectors\n",
    "\n",
    "    def persist(self, path: str = 'storage'):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        with open(f\"{path}/doecment.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.document, f, ensure_ascii=False)\n",
    "        if self.vectors:\n",
    "            with open(f\"{path}/vectors.json\", 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.vectors, f)\n",
    "\n",
    "    def load_vector(self, path: str = 'storage'):\n",
    "        with open(f\"{path}/vectors.json\", 'r', encoding='utf-8') as f:\n",
    "            self.vectors = json.load(f)\n",
    "        with open(f\"{path}/doecment.json\", 'r', encoding='utf-8') as f:\n",
    "            self.document = json.load(f)\n",
    "\n",
    "    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:\n",
    "        return BaseEmbeddings.cosine_similarity(vector1, vector2)\n",
    "\n",
    "    def generateIndexFlatL2(self):\n",
    "        dim, measure = 768, faiss.METRIC_L2\n",
    "        param = 'Flat'\n",
    "        vectors = np.array([vector for vector in self.vectors]).astype('float32')\n",
    "        index = faiss.index_factory(dim, param, measure)\n",
    "        index.add(vectors)  # 将向量库中的向量加入到index中\n",
    "        return index\n",
    "\n",
    "\n",
    "    def query(self, query: str, EmbeddingModel: BaseEmbeddings, k: int = 1) -> List[str]:\n",
    "\n",
    "        start_time = time.time()\n",
    "        index = self.generateIndexFlatL2()\n",
    "        end_time = time.time()\n",
    "        print(' 构建索引 cost %f second' % (end_time - start_time))\n",
    "        query_vector = np.array([EmbeddingModel.get_embedding(query)]).astype(\"float32\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        D, I = index.search(query_vector, k)  # xq为待检索向量，返回的I为每个待检索query最相似TopK的索引list，D为其对应的距离\n",
    "        print(' 检索 cost %f second' % (time.time() - end_time))\n",
    "        return np.array(self.document)[I[0,:]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = FaissVectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.load_vector('../../storage/history_24_1') # 加载本地的数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector.vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、定义embedding算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/paddlenlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/anaconda3/envs/paddlenlp/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[32m[2024-06-25 23:07:15,803] [    INFO]\u001b[0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'rocketqa-zh-base-query-encoder'.\u001b[0m\n",
      "\u001b[32m[2024-06-25 23:07:15,803] [    INFO]\u001b[0m - Already cached /workspace/liujing/.paddlenlp/models/rocketqa-zh-base-query-encoder/ernie_3.0_base_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2024-06-25 23:07:15,833] [    INFO]\u001b[0m - tokenizer config file saved in /workspace/liujing/.paddlenlp/models/rocketqa-zh-base-query-encoder/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2024-06-25 23:07:15,834] [    INFO]\u001b[0m - Special tokens file saved in /workspace/liujing/.paddlenlp/models/rocketqa-zh-base-query-encoder/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from RAG.PaddleEmbedding import PaddleEmbedding\n",
    "embedding = PaddleEmbedding() # 创建EmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 构建索引 cost 0.373451 second\n",
      " 检索 cost 0.043894 second\n",
      "知识库输出：考察周朝各诸侯国的先后关系，作《十二诸侯年表》第二。\n",
      "　　春秋以后，陪臣执政，强国之君竞相称王，及至秦王嬴政，终于吞并各国，铲除封地，独享尊号。\n",
      "　　作《六国年表》第三。\n",
      "　　秦帝暴虐，楚人陈胜发难，项氏大乱秦朝，汉王仗义征伐。\n",
      "　　八年之间，天下三易其主，事变繁多，所以详著《秦楚之际月表》第四。\n",
      "　　汉朝兴建以来，直到太初一百年间，诸侯废立分割的情况，谱录记载不明，史官也无法继续记载，但可据其世系推知其强弱的原由。　　作《汉兴以来诸侯年表》第五。\n",
      "　　高祖始取天下，辅佐他创业的功臣，都得到剖符封爵，恩泽传给他们的后世子孙；有的忘其亲疏远近，分不出辈份，有的竟至杀身亡国。\n",
      "　　作《高祖功臣侯者年表》第六。\n",
      "　　惠帝、景帝年间，增封功臣宗属爵位和食邑。\n",
      "　　作《惠、景间侯者年表》第七。\n",
      "　　北面攻打强悍的匈奴，南面诛讨强劲的越人，征讨四方夷蛮，不少人以武功封侯。\n",
      "　　作《建元以来侯者年表》第八。\n",
      "　　诸侯国日渐强大，吴楚等七国南北连成一片，诸侯王子弟众多，没有爵位封邑，朝廷下令推行恩义，分封诸侯王子弟为侯，致使王国势力日益削弱，而德义却归于朝廷。\n",
      "　　作《王子侯者年表》第九。\n",
      "　　国家的贤相良将，是民众的表率。\n",
      "　　曾看到汉兴以来将相名臣年表，对贤者则记其治绩，对不贤者则明其劣迹。\n",
      "　　作《汉兴以来将相名臣年表》第十。\n",
      "　　夏、商、周三代之礼，各有增减，但总的来看其要领都在于使礼切近人性，通于王道，所以礼根据人的质朴本性而制成，去掉那些繁文缛节，大体顺应了古今之变。\n",
      "\n",
      "　　作《礼书》第一。\n",
      "　　乐是用来移风易俗的。\n",
      "　　自《雅》、《颂》之声兴起，人们就已经喜好郑卫之音，由来已久了。\n",
      "　　被人情所感发，那远方异域之人，就会归附。\n",
      "　　仿照已有《乐书》来论述自古以来音乐的兴衰。\n",
      "　　作《乐书》第二。\n",
      "　　史记没有军队，国家不会强盛；没有德政，国家就不会兴旺。\n",
      "　　黄帝、商汤、周武王以明于此而兴，夏桀、商纣、秦二世以昧于此而亡，怎么可以对此不慎重呢？《司马法》产生已很久了。\n",
      "　　姜太公、孙武、吴起、王子成甫能继承并有所发明，切合近世情况，极尽人世之变。\n",
      "　　作《律书》第三。\n",
      "　　乐律处于阴而治阳，历法处于阳而治阴，律历交替相治，其间不允许丝毫差错。\n",
      "　　原有五家的历书相互悖逆不同，只有太初元年所论历法为是。\n",
      "　　作《历书》第四。\n",
      "　　星气之书，杂有许多求福去灾，预兆吉凶的内容，荒诞不经；推究其文辞，考察其应验，并无什么特别之处。\n",
      "　　等到武帝召集专人研讨此事，并依次用轨度加以验证。\n",
      "　　作《天官书》第五。\n",
      "　　承受天命做了帝王，封禅这样的符瑞之事不可轻易举行，如果举行，那一切神灵没有不受祭祀的F。\n",
      "　　追溯祭祀名山大川诸神之礼，作《封禅书》第六。\n",
      "　　大禹疏通河川，九州得以安宁；及至建立宣防宫之时，河道沟渠更被疏竣。\n",
      "　　作《河渠书》第七。\n",
      "　　钱币的流通，是为沟通农商；其弊端竟发展到玩弄技巧，兼并发财，争相投机牟利，舍本逐末，弃农经商。\n",
      "　　作《平准书》来考察事情的变化发展，这是第八。\n",
      "　　太伯为让季历继位，避居江南蛮夷之地，文王、武王才得以振兴周邦，发展了古公..父的王业。\n",
      "\n",
      "秦二世指的是秦朝的第二个皇帝，他的名字叫胡亥。他是秦始皇的儿子，因为始皇晚年宠爱赵高，赵高与丞相李斯合谋，伪造遗诏，立胡亥为太子。秦二世即位后，因其暴虐无道，导致秦朝社会矛盾激化，最终引发了大规模的农民起义，如陈胜、吴广起义，加速了秦朝的灭亡。他的统治时期是公元前210年至公元前207年，仅持续了三年。\n"
     ]
    }
   ],
   "source": [
    "question = '介绍一下秦二世'\n",
    "content = vector.query(question, EmbeddingModel=embedding, k=3)[0]\n",
    "print(f'知识库输出：{content}')\n",
    "chat = QwenChat()\n",
    "print(chat.chat(question, [], content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
